{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be run once to setup modules and paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload modules without shutting notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "from datetime import date\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global variables including folders\n",
    "home = os.path.expanduser(\"~\")\n",
    "basefolder = os.path.join(home, \"my_shared_data_folder\")\n",
    "s1ardfolder = os.path.join(basefolder, \"s1ard\")\n",
    "datasets = os.path.join(basefolder, \"datasets\")\n",
    "\n",
    "# Upland burn modules\n",
    "import sys\n",
    "sys.path.append(os.path.join(home, \"notebooks/utils\"))\n",
    "import extract_aoi as extract\n",
    "import plot_data as plotd\n",
    "import get_configuration as getc\n",
    "import portal_credentials as portalc\n",
    "import call_cophub as callc\n",
    "import onda_archive as onda\n",
    "import syscmds as sysc\n",
    "import run_gpt as rgpt\n",
    "\n",
    "## setup and clear out temp folder\n",
    "tmpfolder = os.path.join(home, \"temp\")\n",
    "if not os.path.exists(tmpfolder):\n",
    "    os.mkdir(tmpfolder)\n",
    "#else:\n",
    "#    shutil.rmtree(tmpfolder)\n",
    "#    os.mkdir(tmpfolder)\n",
    "\n",
    "# Get configuration information\n",
    "cstudy = \"skye\" \n",
    "#cstudy = \"cairngorms\" \n",
    "#cstudy = \"pdistrict\" \n",
    "\n",
    "basefolder, s1ardfolder, datasets, outfolder, tmpfolder, ofiles, hdfile, pfile, verbose, graphics = getc.get_configuration(cstudy)\n",
    "if cstudy == \"skye\":\n",
    "    aoi = os.path.join(datasets, \"Skye_extent_OSGB36.geojson\")\n",
    "elif cstudy == \"cairngorms\":\n",
    "    aoi = os.path.join(datasets, \"Cairngorms_extent_OSGB36.geojson\")\n",
    "else:\n",
    "    aoi = os.path.join(datasets, \"PDistrict_extent_OSGB36.geojson\")\n",
    "fstart = os.path.basename(aoi).split(\"_\")[0]\n",
    "\n",
    "# Load Copernicus hub credentials\n",
    "pfile = \"cophub.txt\"\n",
    "home = os.path.expanduser(\"~\")\n",
    "if not os.path.exists(os.path.join(home, pfile)):\n",
    "    portalc.save_credentials(pfile)\n",
    "cop_credentials = portalc.read_credentials(pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define date range and search Copernicus Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2019, 1, 1)\n",
    "end_date = date(2019, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call copernicus hub to get list of products\n",
    "products, wkt = callc.call_cophub(cop_credentials, start_date, end_date, aoi, SLC = False)\n",
    "print(\"Found {} products\".format(len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Copernicus Sentinel-1 GRD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI\n",
    "# Log into Copernicus Hub\n",
    "api = SentinelAPI(cop_credentials[0], cop_credentials[1], 'https://scihub.copernicus.eu/dhus')\n",
    "\n",
    "print(\"Downloading GRD files\")\n",
    "s1grdfolder = os.path.join(basefolder, \"s1grd\") \n",
    "grdfiles = []\n",
    "for download_num, (id1, data) in enumerate(products.iterrows()):\n",
    "    filename = data['title']\n",
    "    s1file = os.path.join(s1grdfolder, filename + '.zip')\n",
    "    splits = filename.split(\"_\")\n",
    "    dstring = splits[4]\n",
    "    dstring2 = splits[5]\n",
    "    cedafile = splits[0] + \"_\" + dstring[0:8] + \"_*_\" + dstring[9:15] + \"_\" + dstring2[9:15] + \"_*.tif\"\n",
    "    test = glob.glob(os.path.join(s1ardfolder, cedafile))\n",
    "    # Check if already stored\n",
    "    if (len(test) > 0) or os.path.exists(s1file):\n",
    "        print(\"{} already downloaded\".format(filename))\n",
    "        grdfiles.append(s1file)\n",
    "    else:\n",
    "        info = api.get_product_odata(id1)\n",
    "        url = info['url'] + '/Online/$value'\n",
    "        cmd = 'curl -u ' + cop_credentials[0] + ':' + cop_credentials[1] + ' \"' + url + '\"'\n",
    "        status = sysc.execmd(cmd, verb=False)   \n",
    "        if \"false</Online>\" in status.decode(encoding='UTF-8'): # offline\n",
    "            if download_num == 0:\n",
    "                # Load ONDA credentials\n",
    "                pfile = \"onda.txt\"\n",
    "                home = os.path.expanduser(\"~\")\n",
    "                if not os.path.exists(os.path.join(home, pfile)):\n",
    "                    portalc.save_credentials(pfile)\n",
    "                onda_credentials = portalc.read_credentials(pfile)\n",
    "\n",
    "            # Try to download all products from ONDA if archive file\n",
    "            onda.batch_download_from_archive(onda_credentials, products, s1grdfolder)\n",
    "            \n",
    "            if os.path.exists(s1file):\n",
    "                print(\"{} downloaded\".format(filename))\n",
    "                grdfiles.append(s1file)\n",
    "            \n",
    "        else: # Try from Copernicus hub\n",
    "            print(\"Downloading from Copernicus Hub\")\n",
    "            api.download(id1, directory_path=s1grdfolder)\n",
    "            \n",
    "            if os.path.exists(s1file):\n",
    "                print(\"{} downloaded\".format(filename))\n",
    "                grdfiles.append(s1file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing GRD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the individual subswaths\n",
    "import subprocess\n",
    "xml_folder = os.path.join(home, \"notebooks/xml-files\")\n",
    "calib_folder = os.path.join(s1grdfolder, \"calibrate\")\n",
    "if not os.path.exists(calib_folder):\n",
    "    os.mkdir(calib_folder)\n",
    "\n",
    "count = 0\n",
    "calfiles = []\n",
    "for infile in grdfiles:\n",
    "    basestem = os.path.splitext(os.path.basename(infile))[0]\n",
    "    print(\"Running for {}\".format(basestem))\n",
    "\n",
    "    # Setup properties and xml file for calibration step\n",
    "    xml_file = os.path.join(xml_folder, 'Calibrate.xml')\n",
    "    properties_file = xml_file.replace(\".xml\",\".properties\")\n",
    "\n",
    "    calibfile = os.path.join(calib_folder, basestem + \"_calibrate.dim\")\n",
    "    if os.path.exists(calibfile):\n",
    "        print(\"{} processed to calibration stage\".format(calibfile))\n",
    "\n",
    "    else:    \n",
    "        rgpt.run_gpt(infile,calibfile,xml_file,properties_file)\n",
    "    count += 1\n",
    "    calfiles.append(basestem)\n",
    "    if count == 2:\n",
    "        break  \n",
    "        \n",
    "temporal = True\n",
    "if not temporal:\n",
    "    for basestem in calfiles:\n",
    "        # Setup properties and xml file for terrain correction and filtering step, including subsetting to aoi, using single Lee\n",
    "        xml_file = os.path.join(xml_folder, 'Terrain-Subset.xml')\n",
    "        properties_file = xml_file.replace(\".xml\",\".properties\")\n",
    "\n",
    "        terrfile = os.path.join(calib_folder, basestem + \"_terrain.dim\")\n",
    "        if os.path.exists(terrfile):\n",
    "            print(\"{} processed to terrain stage\".format(terrfile))\n",
    "\n",
    "        else:    \n",
    "            rgpt.run_gpt(calibfile,terrfile,xml_file,properties_file, subset = wkt)\n",
    "\n",
    "        # Setup properties and xml file for reprojection and conversion to GeoTIFF\n",
    "        xml_file = os.path.join(xml_folder, 'Reproject.xml')\n",
    "        properties_file = xml_file.replace(\".xml\",\".properties\")\n",
    "\n",
    "        outfile = os.path.join(calib_folder, basestem + \".tif\")\n",
    "        if os.path.exists(outfile):\n",
    "            print(\"{} processed to final stage\".format(outfile))\n",
    "        else:    \n",
    "            rgpt.run_gpt(terrfile,outfile,xml_file,properties_file)\n",
    "else:\n",
    "    # Setup properties and xml file for terrain correction and filtering step, including subsetting to aoi, using multitemporal\n",
    "    xml_file = os.path.join(xml_folder, 'Terrain-Subset-Collocate.xml')\n",
    "    properties_file = xml_file.replace(\".xml\",\".properties\")\n",
    "    calibfile = os.path.join(calib_folder, calfiles[0] + \"_calibrate.dim\")\n",
    "    calibfile2 = os.path.join(calib_folder, calfiles[1] + \"_calibrate.dim\")\n",
    "\n",
    "    terrfile = os.path.join(calib_folder, calfiles[0] + \"_terrain_2.dim\")\n",
    "    if os.path.exists(terrfile):\n",
    "        print(\"{} processed to terrain stage\".format(terrfile))\n",
    "    else:    \n",
    "        rgpt.run_gpt(calibfile,terrfile,xml_file,properties_file, subset = wkt, collocate = calibfile2)\n",
    "\n",
    "    # Setup properties and xml file for reprojection and conversion to GeoTIFF\n",
    "    xml_file = os.path.join(xml_folder, 'Reproject.xml')\n",
    "    properties_file = xml_file.replace(\".xml\",\".properties\")\n",
    "\n",
    "    outfile = os.path.join(calib_folder, basestem + \".tif\")\n",
    "    if os.path.exists(outfile):\n",
    "        print(\"{} processed to final stage\".format(outfile))\n",
    "    else:    \n",
    "        rgpt.run_gpt(terrfile,outfile,xml_file,properties_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Sentinel-1 ARD data for specific date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ARD data\n",
    "for download_num, (id1, data) in enumerate(products.iterrows()):\n",
    "    filename = data['title']\n",
    "    s1file = filename + '.tif'\n",
    "    splits = filename.split(\"_\")\n",
    "    dstring = splits[4]\n",
    "    dstring2 = splits[5]\n",
    "    cedafile = splits[0] + \"_\" + dstring[0:8] + \"_*_\" + dstring[9:15] + \"_\" + dstring2[9:15] + \"_*.tif\"\n",
    "    test = glob.glob(os.path.join(s1ardfolder, cedafile))\n",
    "    # Check if already stored\n",
    "    if (len(test) > 0) or os.path.exists(s1file):\n",
    "        print(\"{} already downloaded\".format(filename))\n",
    "    else:\n",
    "        if \"Skye\" in aoi or \"Cairngorms\" in aoi: # Download data from JNCC FTP\n",
    "\n",
    "            # Load CEDA FTP credentials\n",
    "            pfile = \"ceda.txt\"\n",
    "            home = os.path.expanduser(\"~\")\n",
    "            if not os.path.exists(os.path.join(home, pfile)):\n",
    "                portalc.save_credentials(pfile)\n",
    "            ceda_credentials = portalc.read_credentials(pfile)\n",
    "\n",
    "            print(\"Querying CEDA FTP\")\n",
    "            ceda = \"ftp://ftp.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_1/\"\n",
    "\n",
    "            # Run curl command to try to download\n",
    "            print(\"Trying to download {} \".format(filename))\n",
    "            urlpath = ceda + os.path.join(dstring[0:4],os.path.join(dstring[4:6], dstring[6:8]))\n",
    "            cmd = 'wget --user ' + ceda_credentials[0] + ' --password ' + ceda_credentials[1] + ' -P ' + s1ardfolder\n",
    "            cmd += ' -r --no-directories --no-host-directories --no-parent -A \"' + cedafile + '\" ' + urlpath\n",
    "            print(\"CMD: \",cmd)\n",
    "            output = sysc.execmd(cmd, verb = False)\n",
    "            if output != None:\n",
    "                size = len(output)\n",
    "                print(output[:size - 30]) # print last 30 characters\n",
    "\n",
    "        elif \"PDistrict\" in aoi:\n",
    "\n",
    "            # Load Defra credentials\n",
    "            pfile = \"defra_api.txt\"\n",
    "            home = os.path.expanduser(\"~\")\n",
    "            if not os.path.exists(os.path.join(home, pfile)):\n",
    "                portalc.save_credentials(pfile)\n",
    "            defra_credentials = portalc.read_credentials(pfile)\n",
    "\n",
    "            print(\"Querying Defra EO Data Service \")\n",
    "            date_value = dstring[0:4] + '-' + dstring[4:6] + '-' + dstring[6:8]\n",
    "            urlpath = 'https://earthobs.defra.gov.uk' + ('/api/layers/?username=' + defra_credentials[0] \n",
    "                                    + '&api_key=' + defra_credentials[1]\n",
    "                                    + '&limit=20000&offset=0'\n",
    "                                    + '&geometry=\"' + wkt.replace(\" \",\"+\") + '\"'\n",
    "                                    + '&date__range=' + date_value + '%2000:00,' + date_value + '%2023:59'\n",
    "                                    + '&title__icontains=' + splits[0]\n",
    "                                    + '&type__in=raster'\n",
    "                                   )\n",
    "            # Making a get request \n",
    "            response = requests.get(urlpath)\n",
    "            print(\"Status: {} {}\".format(response.status_code, response.request))\n",
    "            if response.status_code != 200:\n",
    "                print(\"Failed to download: \", response)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Sentinel-1 ARD data for the Area of Interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of Sentinel-1 ARD (backscatter files)\n",
    "s1ards = glob.glob(os.path.join(s1ardfolder, \"*.tif\"))\n",
    "numfiles  = len(s1ards)\n",
    "print(\"Found {} Sentinel-1 ARD files\".format(numfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset each file to the area of interest\n",
    "ofiles = []\n",
    "print(\"xmin xmax ymin ymax 109880.178605,169814.04887,808556.85961,881457.683008\")\n",
    "if not os.path.exists(aoi):\n",
    "    print(\"Could not find {}\".format(aoi))\n",
    "else:\n",
    "    for ifile in s1ards:\n",
    "        ofile = os.path.join(tmpfolder, os.path.basename(ifile))\n",
    "        \n",
    "        # Delete if exists\n",
    "        #if os.path.exists(ofile):\n",
    "        #    os.remove(ofile)\n",
    "        # Skip if exists\n",
    "        if os.path.exists(ofile):\n",
    "            ofiles.append(ofile)\n",
    "            print(\"{} exists, skipping\".format(os.path.basename(ofile)))\n",
    "        else:\n",
    "            # Create subset in temp folder\n",
    "            extract.cut_by_geojson(ifile, ofile, aoi, verb = True)\n",
    "\n",
    "            if os.path.exists(ofile):\n",
    "                ofiles.append(ofile)\n",
    "                print(\"Generated: \",ofile)\n",
    "\n",
    "print(\"Finished processing, {} output subsets available\".format(len(ofiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for same-day files and merge if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/bin')\n",
    "import gdal_merge as gm\n",
    "\n",
    "# Sort files by names\n",
    "ofiles = sorted(ofiles,key=lambda x: int(os.path.splitext(os.path.basename(x))[0][4:12]))\n",
    "\n",
    "subfiles = []\n",
    "pdatestr = \"\"\n",
    "for ofile in ofiles:\n",
    "    dir, fname = os.path.split(ofile)\n",
    "    fnames = fname.split(\"_\")\n",
    "    datestr = fnames[1]\n",
    "    if datestr == pdatestr:\n",
    "        dir, fname2 = os.path.split(pfile)\n",
    "        if int(fname[4]) < int(fname2[4]):\n",
    "            mfile = os.path.join(dir, fname[:26] + fname2[26:])\n",
    "        else:\n",
    "            mfile = os.path.join(dir, fname2[:26] + fname[26:])\n",
    "        if not os.path.exists(mfile):\n",
    "            print(\"Merging {} and {} to create {}\".format(pfile, ofile, mfile))\n",
    "            shutil.copyfile(ofile,mfile)\n",
    "            gm.main(['', '-init', '0', '-o', mfile, pfile, ofile])\n",
    "            if os.path.exists(mfile):\n",
    "                os.remove(pfile)\n",
    "                os.remove(ofile)        \n",
    "                subfiles.append(mfile)\n",
    "            else:\n",
    "                print(\"Failed to generate merged file {}\".format(mfile))\n",
    "        else:\n",
    "            print(\"Merged file {} already exists\".format(mfile))\n",
    "            \n",
    "    else:\n",
    "        pdatestr = datestr\n",
    "        pfile = ofile\n",
    "        subfiles.append(ofile)\n",
    "# Create final list of files\n",
    "ofiles = []\n",
    "for file in subfiles:\n",
    "    if os.path.exists(file):\n",
    "        ofiles.append(file)\n",
    "print(\"Finished processing, {} output subsets available\".format(len(ofiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display generated subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import ogr, osr, gdal\n",
    "# Enable GDAL/OGR exceptions\n",
    "gdal.UseExceptions()\n",
    "\n",
    "# Setup HDF file to store full sized darray\n",
    "hdfile = os.path.join(tmpfolder, 'skye_s1ard.h5')\n",
    "pfile = os.path.join(tmpfolder, 'skye_s1ard.txt')\n",
    "nfile = os.path.join(tmpfolder, 'skye_s1ard.npy')\n",
    "overwrite = False\n",
    "\n",
    "if overwrite or not os.path.exists(hdfile):\n",
    "    \n",
    "    # Delete existing file\n",
    "    os.remove(hdfile)\n",
    "    \n",
    "    # Setup\n",
    "    darray = 0\n",
    "    layers = 0\n",
    "\n",
    "    with h5py.File(hdfile, 'w') as hf_object:\n",
    "        for ofile in ofiles:\n",
    "            # Open original data as read only\n",
    "            ds = gdal.Open(ofile, gdal.GA_ReadOnly)\n",
    "            bands = ds.RasterCount\n",
    "            print(\"Loading {} into HDF file\".format(os.path.basename(ofile)))\n",
    "            for band in range(bands):\n",
    "                image = ds.GetRasterBand(band+1).ReadAsArray()\n",
    "                if band == 0:\n",
    "                    xdim, ydim = image.shape\n",
    "\n",
    "                    # Getting georeference info\n",
    "                    transform = ds.GetGeoTransform()\n",
    "                    projection = ds.GetProjection()\n",
    "                    xOrigin = transform[0] # top left x \n",
    "                    yOrigin = transform[3] # top left y \n",
    "                    pixelWidth = transform[1] # w-e pixel resolution \n",
    "                    pixelHeight = -transform[5] # n-s pixel resolution (negative value) \n",
    "\n",
    "                if not isinstance(darray, np.ndarray):  # Not an array, setup output data array\n",
    "                    print(\"Setting up darray\")\n",
    "                    #darray = np.zeros((xdim+1, ydim+1, bands, len(ofiles)), dtype=np.float)\n",
    "                    darray = np.zeros((1, bands, xdim+1, ydim+1), dtype=np.float64)\n",
    "                    print(\"darray shape: \",darray.shape)\n",
    "\n",
    "                darray[0, band, 0:image.shape[0],0:image.shape[1]] = image[0:image.shape[0],0:image.shape[1]]\n",
    "\n",
    "                dir, fname = os.path.split(ofile)\n",
    "                print(\"{} Layer {} Band {} min {:.3f} max {:.3f}\".format(fname[4:12],layers,band,np.nanmin(image),np.nanmax(image)))\n",
    "\n",
    "            if layers == 0:\n",
    "                # First write to HDF file\n",
    "                hf_object.create_dataset('s1ard', data=darray, compression=\"gzip\", chunks=(1, bands, xdim+1, ydim+1), maxshape=(None, bands, xdim+1, ydim+1)) \n",
    "            else:\n",
    "                # Append to HDF file\n",
    "                hf_object[\"s1ard\"].resize((hf_object[\"s1ard\"].shape[0] + darray.shape[0]), axis = 0)\n",
    "                hf_object[\"s1ard\"][-darray.shape[0]:] = darray\n",
    "\n",
    "            # Increment layer count    \n",
    "            layers += 1\n",
    "            ds = None\n",
    "\n",
    "    del darray\n",
    "    # Close HDF file\n",
    "    hf_object.close()\n",
    "\n",
    "    print(\"Created stack of data in HDF file: {}\".format(hdfile))\n",
    "else:\n",
    "    print(\"HDF file {} exists\".format(hdfile))\n",
    "    \n",
    "\n",
    "if overwrite or not os.path.exists(pfile):\n",
    "    # Create pickle file to store metadata\n",
    "    layer = 0\n",
    "    # Setup dictionary\n",
    "    data_dict = []\n",
    "    for file in ofiles:\n",
    "        splits = os.path.basename(file).split(\"_\")\n",
    "        data_dict.append({'layer': layer, 'sensor': splits[0], 'date': splits[1], 'stime': splits[4], 'etime': splits[5], \n",
    "                          'polarisation': splits[6], 'rorbit': splits[2], 'direction': splits[3]})\n",
    "        layer += 1\n",
    "\n",
    "    # Write pickle file\n",
    "    outfile = open(pfile,'wb')\n",
    "    pickle.dump(data_dict,outfile)\n",
    "    outfile.close()\n",
    "    print(\"Wrote pickle file to: {}\".format(pfile))\n",
    "    del data_dict\n",
    "else:\n",
    "    print(\"Pickle file {} exists\".format(pfile))\n",
    "        \n",
    "if overwrite or not os.path.exists(nfile):\n",
    "    # Create file to save list of filenames\n",
    "    np.save(nfile, ofiles)\n",
    "    print(\"Wrote list file to: {}\".format(nfile))\n",
    "else:\n",
    "    print(\"List file {} exists\".format(nfile))\n",
    "    \n",
    "# Deploy Skye subset to shared folder\n",
    "skyefolder = os.path.join(basefolder, \"skye\")\n",
    "if not os.path.exists(skyefolder):\n",
    "    os.mkdir(skyefolder)\n",
    "new_hdfile = os.path.join(skyefolder, 'skye_s1ard.h5')\n",
    "new_pfile = os.path.join(skyefolder, 'skye_s1ard.txt')\n",
    "new_nfile = os.path.join(skyefolder, 'skye_s1ard.npy')\n",
    "if overwrite or not os.path.exists(new_hdfile):\n",
    "    shutil.copy(hdfile, new_hdfile)\n",
    "if overwrite or not os.path.exists(new_pfile):\n",
    "    shutil.copy(pfile, new_pfile)\n",
    "if overwrite or not os.path.exists(new_nfile):\n",
    "    shutil.copy(nfile, new_nfile)\n",
    "\n",
    "tfiles = ofiles\n",
    "ofiles = []\n",
    "for file in tfiles:\n",
    "        new_file = os.path.join(skyefolder, os.path.basename(file))\n",
    "        if overwrite or not os.path.exists(new_file):\n",
    "            shutil.copy(file, new_file)\n",
    "        ofiles.append(os.path.basename(file))\n",
    "        \n",
    "print(\"Deployed to {}\".format(skyefolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Read hdf file and display chosen band: VV or VH\n",
    "hf_object = h5py.File(hdfile, 'r')\n",
    "d = hf_object['s1ard']\n",
    "print(\"Displaying {} layers\".format(len(ofiles)))\n",
    "plotd.plot_images(d, ofiles, verb = False, hdf = 'VV')\n",
    "hf_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Write mask array\n",
    "subfile = os.path.join(tmpfolder, \"subfile.tif\")\n",
    "if os.path.exists(subfile):\n",
    "    os.remove(subfile)\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "ds = driver.Create(subfile, subxdim, subydim, 1, gdal.GDT_Float32)\n",
    "ds.SetGeoTransform(new_transform)\n",
    "ds.SetProjection(projection)\n",
    "ds.GetRasterBand(1).SetNoDataValue(0)\n",
    "#ds.GetRasterBand(1).WriteArray(sdata)\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rsgislib_dev]",
   "language": "python",
   "name": "conda-env-rsgislib_dev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
