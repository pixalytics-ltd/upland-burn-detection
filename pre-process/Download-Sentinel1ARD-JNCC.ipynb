{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be run once to setup modules and paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload modules without shutting notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from datetime import date\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import Dropdown, interact, interactive, fixed, interact_manual\n",
    "import sys\n",
    "sys.path.append('/usr/bin')\n",
    "import gdal_merge as gm\n",
    "\n",
    "# Do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global variables including folders\n",
    "home = os.path.expanduser(\"~\")\n",
    "notebfolder = os.path.join(home, \"notebooks\")\n",
    "basefolder = os.path.join(home, \"my_shared_data_folder\")\n",
    "s1ardfolder = os.path.join(basefolder, \"s1ard\")\n",
    "datasets = os.path.join(basefolder, \"datasets\")\n",
    "\n",
    "# Upland burn modules\n",
    "sys.path.append(os.path.join(notebfolder, \"utils\"))\n",
    "import extract_aoi as extract\n",
    "import get_configuration as getc\n",
    "import portal_credentials as portalc\n",
    "import call_cophub as callc\n",
    "import onda_archive as onda\n",
    "import syscmds as sysc\n",
    "\n",
    "## setup temp folder\n",
    "tmpfolder = os.path.join(home, \"temp\")\n",
    "if not os.path.exists(tmpfolder):\n",
    "    os.mkdir(tmpfolder)\n",
    "tmptiffolder = os.path.join(tmpfolder, \"tiffs\")\n",
    "if not os.path.exists(tmptiffolder):\n",
    "    os.mkdir(tmptiffolder)\n",
    "\n",
    "# Create a widget that can be used the choose the case study area of interest\n",
    "list_of_cstudy = [\"skye\", \"cairngorms\", \"pdistrict\"]\n",
    "cstudy_widget = Dropdown(options = list_of_cstudy, description = \"Area:\")\n",
    "def change_cstudy(*args):\n",
    "    print(\"Set to {}\".format(cstudy_widget.value))\n",
    "    \n",
    "cstudy_widget.observe(change_cstudy, 'value')\n",
    "print(\"Choose the case study area\")\n",
    "display(cstudy_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup folders\n",
    "basefolder, s1ardfolder, datasets, outfolder, tmpfolder, ofiles, hdfile, pfile, verbose, graphics = getc.get_configuration(cstudy)\n",
    "if cstudy == \"skye\":\n",
    "    aoi = os.path.join(datasets, \"Skye_extent_OSGB36.geojson\")\n",
    "elif cstudy == \"cairngorms\":\n",
    "    aoi = os.path.join(datasets, \"Cairngorms_extent_OSGB36-extended.geojson\")\n",
    "else:\n",
    "    aoi = os.path.join(datasets, \"PDistrict_extent_OSGB36.geojson\")\n",
    "fstart = os.path.basename(aoi).split(\"_\")[0]\n",
    "aoifolder = os.path.join(basefolder, cstudy)\n",
    "\n",
    "# Load Copernicus hub credentials\n",
    "pfile = \"cophub.txt\"\n",
    "home = os.path.expanduser(\"~\")\n",
    "if not os.path.exists(os.path.join(home, pfile)):\n",
    "    portalc.save_credentials(pfile)\n",
    "cop_credentials = portalc.read_credentials(pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define date range and search Copernicus Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDistrict - April to August 2018\n",
    "# Cairngorms - March to mid June 2019\n",
    "# Skye - Feb to April 2018\n",
    "start_date = date(2018, 2, 1)\n",
    "end_date = date(2018, 4, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call copernicus hub to get list of products\n",
    "products, wkt = callc.call_cophub(cop_credentials, start_date, end_date, aoi, SLC = False)\n",
    "print(\"Found {} products for {} to {} for {}\".format(len(products),start_date,end_date,cstudy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Sentinel-1 ARD data for specific date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ARD data\n",
    "for download_num, (id1, data) in enumerate(products.iterrows()):\n",
    "    filename = data['title']\n",
    "    s1file = filename + '.tif'\n",
    "    splits = filename.split(\"_\")\n",
    "    dstring = splits[4]\n",
    "    dstring2 = splits[5]\n",
    "    cedafile = splits[0] + \"_\" + dstring[0:8] + \"_*_\" + dstring[9:15] + \"_\" + dstring2[9:15] + \"_*.tif\"\n",
    "    test = glob.glob(os.path.join(s1ardfolder, cedafile))\n",
    "    # Check if already stored\n",
    "    if (len(test) > 0) or os.path.exists(s1file):\n",
    "        print(\"{} already downloaded\".format(filename))\n",
    "    else:\n",
    "        # Load CEDA FTP credentials\n",
    "        pfile = \"ceda.txt\"\n",
    "        home = os.path.expanduser(\"~\")\n",
    "        if not os.path.exists(os.path.join(home, pfile)):\n",
    "            portalc.save_credentials(pfile)\n",
    "        ceda_credentials = portalc.read_credentials(pfile)\n",
    "\n",
    "        print(\"Querying CEDA FTP\")\n",
    "        ceda = \"ftp://ftp.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_1/\"\n",
    "\n",
    "        # Run curl command to try to download\n",
    "        print(\"Trying to download {} \".format(filename))\n",
    "        urlpath = ceda + os.path.join(dstring[0:4],os.path.join(dstring[4:6], dstring[6:8]))\n",
    "        cmd = 'wget --user ' + ceda_credentials[0] + ' --password ' + ceda_credentials[1] + ' -P ' + s1ardfolder\n",
    "        cmd += ' -r --no-directories --no-host-directories --no-parent -A \"' + cedafile + '\" ' + urlpath\n",
    "        print(\"CMD: \",cmd)\n",
    "        output = sysc.execmd(cmd, verb = False)\n",
    "        if output != None:\n",
    "            size = len(output)\n",
    "            print(output[:size - 30]) # print last 30 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Sentinel-1 ARD data for the Area of Interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of Sentinel-1 ARD (backscatter files)\n",
    "s1ards = glob.glob(os.path.join(s1ardfolder, \"*.tif\")) # Edit to specify time range\n",
    "numfiles  = len(s1ards)\n",
    "print(\"Found {} Sentinel-1 ARD files\".format(numfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset each file to the area of interest\n",
    "ofiles = []\n",
    "print(\"Generating for {}\".format(aoi))\n",
    "if not os.path.exists(aoi):\n",
    "    print(\"Could not find {}\".format(aoi))\n",
    "else:\n",
    "    for ifile in s1ards:\n",
    "        ofile = os.path.join(tmptiffolder, os.path.basename(ifile))\n",
    "        \n",
    "        # Skip if exists\n",
    "        if os.path.exists(ofile):\n",
    "            ofiles.append(ofile)\n",
    "            print(\"{} exists, skipping\".format(os.path.basename(ofile)))\n",
    "        else:\n",
    "            # Create subset in temp folder\n",
    "            extract.cut_by_geojson(ifile, ofile, aoi, verb = True)\n",
    "\n",
    "            if os.path.exists(ofile):\n",
    "                ofiles.append(ofile)\n",
    "                print(\"Generated: \",ofile)\n",
    "\n",
    "print(\"Finished processing, {} output subsets available for {}\".format(len(ofiles), cstudy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for same-day files and merge if they exist\n",
    "## One generated the subset is manually moved to ~/my_shared_data_folder/datasets/[cstudy]/backscatter_tiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort files by names\n",
    "ofiles = glob.glob(os.path.join(tmptiffolder, \"*.tif\")) # When running from pre-processed data in the temp directory\n",
    "ofiles = sorted(ofiles,key=lambda x: int(os.path.splitext(os.path.basename(x))[0][4:12]))\n",
    "\n",
    "subfiles = []\n",
    "pdatestr = \"\"\n",
    "for ofile in ofiles:\n",
    "    dir, fname = os.path.split(ofile)\n",
    "    fnames = fname.split(\"_\")\n",
    "    datestr = fnames[1]\n",
    "    timestr = fnames[4]\n",
    "    #print(\"Checking {} {} {}\".format(fname, datestr, timestr))\n",
    "    if datestr == pdatestr:\n",
    "        merge = False\n",
    "        dir, fname2 = os.path.split(pfile)\n",
    "        if int(timestr) < int(ptimestr) and int(timestr[0:2]) == int(ptimestr[0:2]):\n",
    "            mfile = os.path.join(dir, fname[:28] + fname2[28:])\n",
    "            merge = True\n",
    "        elif int(timestr) > int(ptimestr) and int(timestr[0:2]) == int(ptimestr[0:2]):\n",
    "            mfile = os.path.join(dir, fname2[:28] + fname[28:])\n",
    "            merge = True\n",
    "        if merge and not os.path.exists(mfile) and os.path.exists(ofile) and os.path.exists(pfile):\n",
    "            print(\"Merging {} and {} to create {}\".format(pfile, ofile, mfile))\n",
    "            shutil.copyfile(ofile,mfile)\n",
    "            gm.main(['', '-init', '0', '-o', mfile, pfile, ofile])\n",
    "            if os.path.exists(mfile):\n",
    "                os.remove(pfile)\n",
    "                os.remove(ofile)        \n",
    "                subfiles.append(mfile)\n",
    "            else:\n",
    "                print(\"Failed to generate merged file {}\".format(mfile))\n",
    "        else:\n",
    "            print(\"Merged file {} already exists\".format(mfile))\n",
    "            if os.path.exists(pfile):\n",
    "                os.remove(pfile)\n",
    "            if os.path.exists(ofile):\n",
    "                os.remove(ofile)        \n",
    "            subfiles.append(mfile)\n",
    "            \n",
    "    else:\n",
    "        pdatestr = datestr\n",
    "        ptimestr = timestr\n",
    "        pfile = ofile\n",
    "        subfiles.append(ofile)\n",
    "\n",
    "# Create final list of files\n",
    "ofiles = []\n",
    "for file in subfiles:\n",
    "    if os.path.exists(file):\n",
    "        ofiles.append(file)\n",
    "print(\"Finished processing, {} output subsets available for {}\".format(len(ofiles), cstudy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rsgislib_dev]",
   "language": "python",
   "name": "conda-env-rsgislib_dev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
