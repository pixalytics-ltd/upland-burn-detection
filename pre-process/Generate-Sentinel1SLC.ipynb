{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for downloading and processing Sentinel-1 coherence data\n",
    "The first 2 cells initiate the notebook and import the required modules, before allowing you to select the case study area. The first cell only needs to be run once to setup modules and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload modules without shutting notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import date\n",
    "import subprocess\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import Dropdown, interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "sys.path.append('/usr/bin')\n",
    "import gdal_merge as gm\n",
    "import gdal\n",
    "\n",
    "# Do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global variables including folders\n",
    "home = os.path.expanduser(\"~\")\n",
    "notebfolder = os.path.join(home, \"notebooks\")\n",
    "basefolder = os.path.join(home, \"my_shared_data_folder\")\n",
    "s1slcfolder = os.path.join(basefolder, \"s1slc\")\n",
    "datasets = os.path.join(basefolder, \"datasets\")\n",
    "\n",
    "# Upland burn modules\n",
    "sys.path.append(os.path.join(notebfolder, \"utils\"))\n",
    "import extract_aoi as extract\n",
    "import portal_credentials as portalc\n",
    "import call_cophub as callc\n",
    "import onda_archive as onda\n",
    "import functions as func\n",
    "\n",
    "# Create a widget that can be used the choose the case study area of interest\n",
    "list_of_cstudy = [\"skye\", \"cairngorms\", \"pdistrict\"]\n",
    "cstudy_widget = Dropdown(options = list_of_cstudy, description = \"Area:\")\n",
    "def change_cstudy(*args):\n",
    "    print(\"Set to {}\".format(cstudy_widget.value))\n",
    "    \n",
    "cstudy_widget.observe(change_cstudy, 'value')\n",
    "print(\"Choose the case study area\")\n",
    "display(cstudy_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstudy = cstudy_widget.value\n",
    "\n",
    "if cstudy == \"skye\":\n",
    "    aoi = os.path.join(datasets, \"Skye_extent_OSGB36.geojson\")\n",
    "elif cstudy == \"cairngorms\":\n",
    "    # aoi = os.path.join(datasets, \"Cairngorms_extent_OSGB36.geojson\") # Original smaller area\n",
    "    aoi = os.path.join(datasets, \"Cairngorms_extent_OSGB36-extended.geojson\")\n",
    "else:\n",
    "    aoi = os.path.join(datasets, \"PDistrict_extent_OSGB36.geojson\")\n",
    "fstart = os.path.basename(aoi).split(\"_\")[0]\n",
    "\n",
    "# Load Copernicus hub credentials\n",
    "pfile = \"cophub.txt\"\n",
    "home = os.path.expanduser(\"~\")\n",
    "## setup and clear out temp folder\n",
    "tmpfolder = os.path.join(home, \"temp\")\n",
    "if not os.path.exists(tmpfolder):\n",
    "    os.mkdir(tmpfolder)\n",
    "else:\n",
    "    shutil.rmtree(tmpfolder)\n",
    "    os.mkdir(tmpfolder)\n",
    "\n",
    "if not os.path.exists(os.path.join(home, pfile)):\n",
    "    portalc.save_credentials(pfile)\n",
    "cop_credentials = portalc.read_credentials(pfile)\n",
    "final_folder = os.path.join(basefolder, \"%s/coherence_tiffs\"%(cstudy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define date range and search Copernicus Hub\n",
    "Once the case study area is chosen, the start and end dates for the area to process will be set (although they can be overridden at the end of the first cell). These dates and the area of interest are then passed to the API and all the potential Sentinel-1 products returned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date ranges - the ones below are already processed. Alternatively unhash the last 2 lines and set your own range\n",
    "if cstudy == \"skye\":\n",
    "    start_date = date(2018, 2, 1)\n",
    "    end_date = date(2018, 4, 30)\n",
    "elif cstudy == \"cairngorms\":\n",
    "    start_date = date(2019, 3, 1)\n",
    "    end_date = date(2019, 5, 31)\n",
    "else:\n",
    "    start_date = date(2018, 4, 1)\n",
    "    end_date = date(2018, 8, 31)\n",
    "\n",
    "#start_date = date(2018, 4, 1)\n",
    "#end_date = date(2018, 8, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call copernicus hub to get list of products\n",
    "products, wkt2 = callc.call_cophub(cop_credentials, start_date, end_date, aoi, SLC = True)\n",
    "print(\"Found {} products\".format(len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work out pairings for analysis and filter any already processed files out\n",
    "Running the cell below will allow you to choose a specific orbit that you want to process. Note, at this point it is worth checking how well the orbits overlap your area of interest, as the list of products will include any orbits that just clip the edge of your area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the available orbits\n",
    "list_of_orbits = products['relativeorbitnumber'].to_list()\n",
    "# Create a widget that can be used the choose the polygon of interest\n",
    "orb_widget = Dropdown(options = list(set(list_of_orbits)), description = \"Orbit:\")#, value = list(poly_dict.keys())[0])\n",
    "def change_orbit(*args):\n",
    "    print(\"Set to {}\".format(orb_widget.value))\n",
    "    \n",
    "orb_widget.observe(change_orbit, 'value')\n",
    "print(\"Choose orbit you wish to analyse. Its probably worth checking it covers an appropriate amount of your area of interest\")\n",
    "display(orb_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates lists and dictionaries based on your chosen data, that will determine which, if any, of the products for your chosen area and orbit need downloading and or processing. It does this by back determining what intitial and intermediate products required for the final products, so if the final product exists it simply skips them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_potential_pairs = []\n",
    "pairing_coherence_dict = {}\n",
    "file_merge_dict = {}\n",
    "processed_folder = os.getcwd()# os.path.join(home, 'my_shared_data_folder/s1slc/Processed_images/')\n",
    "times_needed = {}\n",
    "\n",
    "for orb in set(list_of_orbits): # loop through the orbits available\n",
    "    if orb != orb_widget.value:\n",
    "        continue\n",
    "    orb_products = products[products[\"relativeorbitnumber\"]==orb] # get only the products matching the current orbit\n",
    "    for id1, p1 in orb_products.iterrows():\n",
    "        pairs = []\n",
    "        filtered_data = func.filter_dataframe_by_time(orb_products, p1, max_time=8) # Lets filter by time\n",
    "        filtered_data = func.filter_dataframe_by_overlap(filtered_data, id1, min_overlap=20) # Lets filter by overlap\n",
    "        if len(filtered_data) == 0:\n",
    "            continue\n",
    "        for id2, data in filtered_data.iterrows():\n",
    "            if p1['beginposition'] > data['beginposition']: # we will only use the files that are at a later date as not to duplicate pairings\n",
    "                continue\n",
    "            pairs = [p1['title'] + \".zip\", data['title'] + \".zip\"]\n",
    "            list_of_potential_pairs.append(pairs)\n",
    "\n",
    "\n",
    "            # we need to create the merge dictionary for later \n",
    "            basename = os.path.basename(p1['title']).strip(\".zip\")\n",
    "            orbit_direction = products[products['title'] == basename]['orbitdirection'][0]\n",
    "            file1 = os.path.basename(p1['title']).split(\"_\")[-5]\n",
    "            file2 = os.path.basename(data['title']).split(\"_\")[-5]\n",
    "\n",
    "            for pol in [\"VV\", \"VH\"]: \n",
    "                # lets note down how many times original files are needed\n",
    "                for t in [p1['title'], data['title']]:\n",
    "                    if not t in times_needed.keys():\n",
    "                        times_needed[t] = 1\n",
    "                    else:\n",
    "                        times_needed[t] +=1\n",
    "                # The merge id will be unique and will be the key to access files that were aquired with the same\n",
    "                # relative orbit, sensor, orbit direction, polarisation that were acquired on the same day\n",
    "                \n",
    "                # create the final merged filename\n",
    "                outfile = file1 + \"_\" + file2 + \"_%s.tif\"%(pol)\n",
    "                sensor = os.path.basename(p1['title'])[2] + os.path.basename(data['title'])[2]\n",
    "                merge_id = (str(orb), sensor, outfile[:8], outfile.split(\"_\")[1][:8], orbit_direction, pol, cstudy)\n",
    "                if not merge_id in file_merge_dict.keys():\n",
    "                    file_merge_dict[merge_id] = [outfile]\n",
    "                else:\n",
    "                    file_merge_dict[merge_id].append(outfile)\n",
    "                # use this dictionary to back calculate from final images so we know what we need to download\n",
    "                pairing_coherence_dict[outfile] = [p1['title'], data['title']]\n",
    "\n",
    "already_processed = []\n",
    "combos_not_needed = []\n",
    "# loop through the final folder\n",
    "for file in os.listdir(final_folder):\n",
    "    if int(file.split(\"_\")[0]) != orb:\n",
    "        continue\n",
    "    identifier = (tuple(file.replace(\".tif\", \"\").split(\"_\"))) # get the identifier for the file\n",
    "    processed = file_merge_dict[identifier] # get the list of files that were used for the merge\n",
    "    combos_not_needed.append(processed)\n",
    "    for proc in processed:\n",
    "        orig_files = pairing_coherence_dict[proc] # for each file used in the merge get the original paired images\n",
    "        for f in orig_files:\n",
    "            already_processed.append(f)\n",
    "\n",
    "counts = dict()\n",
    "for i in already_processed: # create a dictionary telling us how many times each original file has been used\n",
    "    counts[i] = counts.get(i, 0) + 1\n",
    "\n",
    "orig_files_not_needed = []\n",
    "for file, num_used in counts.items(): # If the number of times used matches the number of times needed - we don't need to download it!\n",
    "    if num_used == times_needed[file]:\n",
    "        orig_files_not_needed.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Copernicus Sentinel-1 SLC data\n",
    "This cell filters the products based on the chosen information and downloads any original Sentinel-1 SLC files you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into Copernicus Hub\n",
    "\n",
    "print(\"Downloading SLC files\")\n",
    "print(\"Found %s products\"%(len(products)))\n",
    "products_to_download = products[products[\"relativeorbitnumber\"] == orb]\n",
    "products_to_download = func.filter_dataframe_by_file_list(products_to_download, orig_files_not_needed)\n",
    "products_to_download = onda.check_for_existing_downloads(products_to_download, s1slcfolder)\n",
    "print(\"We already have or have processed %s files, so only need to download %s files. Starting download...\"%(len(products) - len(products_to_download) , len(products_to_download)))\n",
    "onda.batch_download_from_archive(cop_credentials, products_to_download, s1slcfolder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing SLC data\n",
    "The next cells initiate the functions required for performing the coherence analysis and then determine the baselines for the pairings.\n",
    "\n",
    "**Note:** ESA's SNAP tool (http://step.esa.int/main/download/snap-download/) needs to be installed an the path to the binary folder specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt(xml_filename, propfile):\n",
    "    \"\"\"\n",
    "    Takes an xml file and a propertie file and processes it using snaps GPT tool\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    xml_filename : path\n",
    "        Path to the xml file\n",
    "    propfile : path\n",
    "        Path to the properties file\n",
    "    \"\"\"\n",
    "    \n",
    "    gpt = '/opt/snap/bin/gpt' # set the path for the SNAP gpt executable\n",
    "    \n",
    "    assert os.path.exists(gpt), \"Unable to find 'gpt' executable.\"\n",
    "    args = [\n",
    "        gpt,\n",
    "        xml_filename,\n",
    "        \"-p\",\n",
    "        propfile\n",
    "    ]\n",
    "\n",
    "    # create the subprocess\n",
    "    p = subprocess.Popen(args,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         bufsize=1,\n",
    "                         universal_newlines=True)\n",
    "\n",
    "    # forward messages from stdout and stderr onto the console\n",
    "    with p.stdout as stdout:\n",
    "        for line in iter(stdout.readline, b\"\"):\n",
    "            if line == \"\":\n",
    "                break\n",
    "            print(line.rstrip())\n",
    "\n",
    "    # wait to exit and retreieve the exit code\n",
    "    exit_code = p.wait()\n",
    "\n",
    "    # raise an exception if 'gpt' return an unexpected exit code\n",
    "    if exit_code != 0:\n",
    "        raise RuntimeError(\"Non-zero return code from GPT.\")\n",
    "        \n",
    "def run_coherence(file1, file2, wkt, polarisation = \"VV\"):\n",
    "    \"\"\"\n",
    "    Creates a coherence image based on 2 overlapping files for an area of interest\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    file1 : path \n",
    "        Path to the first file \n",
    "    file2 : path \n",
    "        Path to the second file \n",
    "    wkt : string\n",
    "        Polygon for the area of interest \n",
    "    polarisation : str, opt\n",
    "        The polarisation band to use. Either \"VV\" or \"VH\" (default : \"VV\")\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    # First lets set the paths for intermediate data and final folders\n",
    "    xml_folder = os.path.join(home, \"notebooks/SLC_Data_processing/XML_files/\")\n",
    "    intermediate_path = os.path.join(home, \"notebooks/SLC_Data_processing/Intermediate_files/\")\n",
    "    processed_folder =  os.path.join(home, 'my_shared_data_folder/s1slc/Processed_images/')\n",
    "    # Create the final output name\n",
    "    outputname = os.path.basename(file1).split(\"_\")[5] + \"_\" + os.path.basename(file2).split(\"_\")[5] + \"_%s\"%(polarisation)\n",
    "    outputname = os.path.join(processed_folder, outputname+\".tif\")                  \n",
    "    \n",
    "    assert os.path.isdir(processed_folder), \"Unable to locate '{}' please ensure this path exists.\".format(processed_folder) \n",
    "    assert os.path.isdir(intermediate_path), \"Unable to locate '{}' please ensure this path exists.\".format(intermediate_path)\n",
    "    \n",
    "    if not (os.path.exists(file1)):\n",
    "        print(\"Unable to locate '%s'.\"%(file1))\n",
    "        return\n",
    "    if not (os.path.exists(file2)):\n",
    "        print(\"Unable to locate '%s'.\"%(file2))\n",
    "        return\n",
    "    \n",
    "    if os.path.isfile(outputname):\n",
    "        print(\"Already analysed pairing %s and %s\"%(file1, file2))\n",
    "        return\n",
    "    try: # lets try and run it!\n",
    "        print(\"Analysing pair: %s and %s\"%(file1, file2))\n",
    "        if os.path.exists(intermediate_path): # clear the intermediate folder in case any old files remain\n",
    "            shutil.rmtree(intermediate_path)\n",
    "        os.mkdir(intermediate_path)\n",
    "        \n",
    "        # First we need to update/create the properties files for each swath \n",
    "        input1 = \"input1=\"+file1\n",
    "        input2 = \"input2=\"+file2\n",
    "        outputs = []\n",
    "        for swathnum in range(1, 4):\n",
    "            output = \"output=\" + intermediate_path + \"IW%s\"%(swathnum)\n",
    "            swath=\"swath=IW%s\"%(swathnum)\n",
    "            polar=\"polarisation=%s\"%(polarisation)\n",
    "            f = open(os.path.join(xml_folder, \"subswath%s.properties\"%(swathnum)), \"w\")\n",
    "            f.write(input1+\"\\n\")\n",
    "            f.write(input2+\"\\n\")\n",
    "            f.write(output+\"\\n\")\n",
    "            f.write(swath+\"\\n\")\n",
    "            f.write(polar+\"\\n\")\n",
    "            f.close()\n",
    "            outputs.append(\"input%s=\"%(swathnum) + intermediate_path + \"IW%s.dim\"%(swathnum))\n",
    "        # now lets write the Mergeswaths.properties file\n",
    "        f = open(os.path.join(xml_folder, \"Mergeswaths.properties\"), \"w\")\n",
    "        f.write(outputs[0]+\"\\n\")\n",
    "        f.write(outputs[1]+\"\\n\")\n",
    "        f.write(outputs[2]+\"\\n\")\n",
    "        f.write(polar+\"\\n\")\n",
    "        f.write(\"output=\" + outputname.strip(\".tif\") + \"\\n\")\n",
    "        f.write(\"polygon=\"+wkt)\n",
    "        f.close()\n",
    "\n",
    "        # finally lets write the subset.properties file\n",
    "        f = open(os.path.join(xml_folder, \"subset.properties\"), \"w\")\n",
    "        dimfile = outputname.strip(\".tif\") + \".dim\"\n",
    "        f.write(\"input1=\" + dimfile + \"\\n\")\n",
    "        f.write(\"output=\" + outputname.strip(\".tif\") + \"\\n\")\n",
    "        f.write(\"polygon=\"+wkt)\n",
    "        f.close()\n",
    "        xml_filename = os.path.join(xml_folder, 'ProcessSubswath.xml')\n",
    "        assert os.path.isfile(xml_filename), \"Unable to locate '{}'.\".format(xml_filename)\n",
    "\n",
    "        #Lets loop through and process the individual swaths\n",
    "        properties_files = [os.path.join(xml_folder, file) for file in os.listdir(xml_folder) if (file.endswith(\".properties\")) and (file.startswith(\"subswath\"))]\n",
    "\n",
    "        # create the subprocess\n",
    "        for propfile in properties_files:\n",
    "            print(\"Running on %s for %s\"%(propfile, outputname))\n",
    "            run_gpt(xml_filename, propfile)\n",
    "            #clear_output(wait=True)\n",
    "\n",
    "        # Merge subswaths\n",
    "        # OLD VERSION of doing together didnt always work, now we subset separately\n",
    "        #print(\"Merging subswaths and saving to %s\"%(outputname))\n",
    "        #xml_filename = os.path.join(xml_folder, 'Mergeswaths_withsubset.xml')\n",
    "        \n",
    "        propfile = os.path.join(xml_folder, 'Mergeswaths.properties')\n",
    "        xml_filename = os.path.join(xml_folder, 'Mergeswaths.xml')\n",
    "        run_gpt(xml_filename, propfile)\n",
    "        \n",
    "        # Subset to the area of interest\n",
    "        print(\"Subsetting subswaths and saving to %s\"%(outputname))\n",
    "        xml_filename = os.path.join(xml_folder, 'subset.xml')\n",
    "        propfile = os.path.join(xml_folder, 'subset.properties')\n",
    "        run_gpt(xml_filename, propfile)\n",
    "        \n",
    "        # Lets clear up the intermediate files\n",
    "        if os.path.exists(dimfile):\n",
    "            os.remove(dimfile)\n",
    "        dimfile = dimfile.strip(\".dim\") + \".data\"\n",
    "        if os.path.exists(dimfile):\n",
    "            shutil.rmtree(dimfile)\n",
    "    except:\n",
    "        if os.path.exists(dimfile):\n",
    "            os.remove(dimfile)\n",
    "        dimfile = dimfile.strip(\".dim\") + \".data\"\n",
    "        if os.path.exists(dimfile):\n",
    "            shutil.rmtree(dimfile)\n",
    "        \n",
    "    clear_output(wait=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %run ./Generate-Sentinel1SLC-Baseline.ipynb $list_of_potential_pairs $cstudy $products\n",
    "except Exception as e:\n",
    "    clear_output(wait=True)\n",
    "    print(\"There was a problem determining the baselines: \", e.__class__, \"occurred. Full error below:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in list_of_potential_pairs:\n",
    "    file1 = os.path.join(s1slcfolder, p[0])\n",
    "    file2 = os.path.join(s1slcfolder, p[1])\n",
    "    for pol in [\"VV\", \"VH\"]:\n",
    "        #continue\n",
    "        run_coherence(file1, file2, wkt2, polarisation = pol) # lets generate the coherence for these images\n",
    "clear_output(wait=True)\n",
    "print(\"Finished analysing the available pairings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1slc_analysed_folder = os.path.join(s1slcfolder, \"Processed_images\")\n",
    "final_folder = os.path.join(basefolder, \"%s/coherence_tiffs\"%(cstudy))\n",
    "assert os.path.exists(final_folder), \"Final folder %s doesn't exist\"%(final_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for same-day files and merge if they exist\n",
    "Finally, the following cells run the coherence processing on each of the pairs before merging together any files that were acquired on the same day by consecutive slices and extracts the final area of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sub_process(args):\n",
    "    \"\"\"\n",
    "    Takes a list of arguments and runs a subprocess in terminal \n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    args : list\n",
    "        list of arguments\n",
    "    \"\"\"\n",
    "    p = subprocess.Popen(args,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT,\n",
    "                         bufsize=1,\n",
    "                         universal_newlines=True)\n",
    "\n",
    "    # forward messagefinal_folderrom stdout and stderr onto the console\n",
    "    with p.stdout as stdout:\n",
    "        for line in iter(stdout.readline, b\"\"):\n",
    "            if line == \"\":\n",
    "                break\n",
    "            print(line.rstrip())\n",
    "\n",
    "    # wait to exit and retreieve the exit code\n",
    "    exit_code = p.wait()\n",
    "\n",
    "    # raise an exception if 'gpt' return an unexpected exit code\n",
    "    if exit_code != 0:\n",
    "        raise RuntimeError(\"Non-zero return code from GPT.\")\n",
    "\n",
    "ofiles = []\n",
    "failed = 0\n",
    "for id1, files in file_merge_dict.items():\n",
    "    # Create the file names for the intermediate and final products\n",
    "    outfile = \"%s_%s_%s_%s_%s_%s_%s\"%(id1[0], id1[1], id1[2], id1[3], id1[4], id1[5], id1[6])\n",
    "    outfile_presub = outfile + \"_presub\"\n",
    "    outfile_presub = os.path.join(final_folder, '%s.tif'%(outfile_presub))\n",
    "    outfile_tif = os.path.join(final_folder, '%s.tif'%(outfile))\n",
    "    before_projection = outfile + \"_nonproj\"\n",
    "    before_projection = os.path.join(final_folder, '%s.tif'%(before_projection))\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(\"Extracting area of interest...\")\n",
    "        extract.cut_by_geojson(files[0], before_projection, aoi, verb = True, slc=True)\n",
    "        os.remove(outfile_presub)\n",
    "        \n",
    "        print(\"Reprojecting %s to %s\"%(before_projection, outfile_tif))\n",
    "        new_res = 10\n",
    "        gdal.Warp(outfile_tif, before_projection, dstSRS=\"EPSG:27700\", dstNodata=0, xRes=new_res, yRes=new_res, format='Gtiff')\n",
    "        os.remove(before_projection)\n",
    "        \n",
    "        ofiles.append(outfile_tif)\n",
    "        continue\n",
    "    if os.path.exists(outfile_tif):\n",
    "        print(\"Already merged %s\"%(outfile_tif))\n",
    "        ofiles.append(outfile_tif)\n",
    "        continue\n",
    "    all_there = True \n",
    "    # lets check we have all the files we need for the correct merge\n",
    "    for f in files:\n",
    "        if not os.path.exists(os.path.join(s1slc_analysed_folder, f)):\n",
    "            all_there = False\n",
    "    if not all_there:\n",
    "        print(\"Not all files available for this merge, skipping.\")\n",
    "        continue            \n",
    "    try:\n",
    "        print(\"Merging {} to create {}\".format(files, outfile_presub))\n",
    "        # build the vrt \n",
    "        args = ['gdalbuildvrt', '-srcnodata', '0', '%s.vrt'%(outfile)]\n",
    "        for file in files:\n",
    "            args.append(os.path.join(s1slc_analysed_folder, file))\n",
    "        # create the subprocess\n",
    "        run_sub_process(args)\n",
    "\n",
    "        # translate files\n",
    "        args = ['gdal_translate', '-of', 'GTiff', '%s.vrt'%(outfile), outfile_presub]\n",
    "        run_sub_process(args)\n",
    "\n",
    "        #delete the vrt \n",
    "        os.remove('%s.vrt'%(outfile))\n",
    "        print(\"Extracting area of interest...\")\n",
    "        extract.cut_by_geojson(outfile_presub, before_projection, aoi, verb = True, slc=True)\n",
    "        os.remove(outfile_presub)\n",
    "        \n",
    "        print(\"Reprojecting %s to %s\"%(before_projection, outfile_tif))\n",
    "        new_res = 10\n",
    "        gdal.Warp(outfile_tif, before_projection, dstSRS=\"EPSG:27700\", dstNodata=0, xRes=new_res, yRes=new_res, format='Gtiff')\n",
    "        os.remove(before_projection)\n",
    "        \n",
    "        ofiles.append(outfile_tif)\n",
    "    \n",
    "    except: # clean up temp / corrupt files\n",
    "        print(\"Failed to generate merged file {}\".format(outfile))\n",
    "        failed +=1\n",
    "        if os.path.exists(outfile_tif):\n",
    "            os.remove(outfile_tif)\n",
    "        if os.path.exists(outfile_presub):\n",
    "            os.remove(outfile_presub)\n",
    "        if os.path.exists('%s.vrt'%(outfile)):\n",
    "            os.remove('%s.vrt'%(outfile))\n",
    "        if os.path.exists(before_projection):\n",
    "            os.remove(before_projection)\n",
    "    \n",
    "clear_output(wait=True)\n",
    "print(\"Finished processing, {} output subsets available\".format(len(ofiles)))        \n",
    "print(\"Failed on %s\"%(failed))\n",
    "print(\"Workbooks currently recognise folders called %1 and %2, so rename as needed or update utils/get_configuration.py\"%(final_folder, final_folder))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rsgislib_dev]",
   "language": "python",
   "name": "conda-env-rsgislib_dev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
